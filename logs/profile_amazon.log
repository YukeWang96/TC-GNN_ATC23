==PROF== Connected to process 135322 (/home/yuke/anaconda3/envs/tcgnn/bin/python3.6)
==PROF== Profiling "vectorized_elementwise_kernel" - 1: 0%....50%....100% - 18 passes
==PROF== Profiling "warmup()" - 2: 0%....50%....100% - 18 passes
==PROF== Profiling "spmm_forward_cuda_kernel" - 3: 0%....50%....100% - 18 passes
Namespace(classes=22, dataset='amazon0505', dim=16, epochs=10, hidden=16, model='gcn', num_layers=2, single_kernel=True, sparsity=1)
Prep. (ms):	641.565
TC_Blocks:	455410
Exp_Edges:	58292480
gflop: 156.124, embedding_dim: 16
TC-GNN -- Time (ms): 939.185, GFLOPs: 0.166
================================
==PROF== Disconnected from process 135322
[135322] python3.6@127.0.0.1
  void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>), 2021-May-23 12:12:01, Context 1, Stream 7
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           9.29
    SM Frequency                                                             cycle/nsecond                           1.36
    Elapsed Cycles                                                                   cycle                         41,330
    Memory [%]                                                                           %                          85.28
    SOL DRAM                                                                             %                          85.28
    Duration                                                                       usecond                          30.34
    SOL L1/TEX Cache                                                                     %                          48.56
    SOL L2 Cache                                                                         %                          41.41
    SM Active Cycles                                                                 cycle                      35,440.18
    SM [%]                                                                               %                           6.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    OK    The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Memory Workload Analysis section.                                         

    OK    The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance.                                      

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.28
    Executed Ipc Elapsed                                                        inst/cycle                           0.24
    Issue Slots Busy                                                                     %                           7.11
    Issued Ipc Active                                                           inst/cycle                           0.28
    SM Busy                                                                              %                           7.94
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   All pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per       
          scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.                 

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                         760.44
    Mem Busy                                                                             %                          41.41
    Max Bandwidth                                                                        %                          85.28
    L1/TEX Hit Rate                                                                      %                              0
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.06
    Mem Pipes Busy                                                                       %                           6.07
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                           7.38
    Issued Warp Per Scheduler                                                                                        0.07
    No Eligible                                                                          %                          92.62
    Active Warps Per Scheduler                                                        warp                           5.37
    Eligible Warps Per Scheduler                                                      warp                           0.08
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 13.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.37 active warps per scheduler, but only an average of 0.08 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps either increase the number of active warps or reduce the time the active warps are stalled.    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          72.82
    Warp Cycles Per Executed Instruction                                             cycle                          73.37
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        32.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 44.2 cycles being stalled waiting after an EXIT instruction for   
          all outstanding memory instructions to complete so that the warp's resources can be freed. This represents    
          about 60.7% of the total average of 72.8 cycles between issuing two instructions. A high number of stalls     
          due to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel.   
          Make sure the memory access patterns of these store operations are optimal for the target architecture and    
          consider parallelized data reduction, if applicable.                                                          
    ----- --------------------------------------------------------------------------------------------------------------
    OK    Check the Source Counters section for the top stall locations in your source based on sampling data.          

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                       2,501.57
    Executed Instructions                                                             inst                        820,516
    Avg. Issued Instructions Per Scheduler                                            inst                       2,520.40
    Issued Instructions                                                               inst                        826,690
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                      25,640
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,640,960
    Waves Per SM                                                                                                    19.54
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                            100
    Block Limit Warps                                                                block                             24
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                          66.67
    Achieved Occupancy                                                                   %                          43.00
    Achieved Active Warps Per SM                                                      warp                          20.64
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is limited by the number of blocks that can fit on the SM. The difference 
          between calculated theoretical and measured achieved occupancy can be the result of warp scheduling           
          overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within  
          a block as well as across blocks of the same kernel.                                                          

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.06
    Branch Instructions                                                               inst                         51,288
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  warmup(), 2021-May-23 12:12:02, Context 1, Stream 7
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           7.87
    SM Frequency                                                             cycle/nsecond                           1.15
    Elapsed Cycles                                                                   cycle                          2,219
    Memory [%]                                                                           %                           1.15
    SOL DRAM                                                                             %                              0
    Duration                                                                       usecond                           1.92
    SOL L1/TEX Cache                                                                     %                         257.14
    SOL L2 Cache                                                                         %                           1.15
    SM Active Cycles                                                                 cycle                           3.50
    SM [%]                                                                               %                           0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    OK    The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance.                                      

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           0.01
    Executed Ipc Elapsed                                                        inst/cycle                           0.00
    Issue Slots Busy                                                                     %                           0.35
    Issued Ipc Active                                                           inst/cycle                           0.01
    SM Busy                                                                              %                           0.35
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   All pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per       
          scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.                 

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                          byte/second                              0
    Mem Busy                                                                             %                           1.15
    Max Bandwidth                                                                        %                           0.41
    L1/TEX Hit Rate                                                                      %                              0
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         135.32
    Mem Pipes Busy                                                                       %                              0
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                           1.42
    Issued Warp Per Scheduler                                                                                        0.01
    No Eligible                                                                          %                          98.58
    Active Warps Per Scheduler                                                        warp                              1
    Eligible Warps Per Scheduler                                                      warp                           0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 70.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, but only an average of 0.01 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps either increase the number of active warps or reduce the time the active warps are stalled.    

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          70.50
    Warp Cycles Per Executed Instruction                                             cycle                            141
    Avg. Active Threads Per Warp                                                                                        1
    Avg. Not Predicated Off Threads Per Warp                                                                            1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 61.2 cycles being stalled waiting for an immediate constant cache 
          (IMC) miss. This represents about 86.9% of the total average of 70.5 cycles between issuing two               
          instructions. A read from constant memory costs one memory read from global memory only on a cache miss;      
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. All threads access the same value.                                          
    ----- --------------------------------------------------------------------------------------------------------------
    OK    Check the Source Counters section for the top stall locations in your source based on sampling data.          
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 1.0 threads being active per cycle.                             

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                           0.01
    Executed Instructions                                                             inst                              2
    Avg. Issued Instructions Per Scheduler                                            inst                           0.01
    Issued Instructions                                                               inst                              4
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                          1
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                              1
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance. This is particularly beneficial to      
          kernels that frequently call __syncthreads().                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources.            

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                            100
    Block Limit Warps                                                                block                             48
    Theoretical Active Warps per SM                                                   warp                             16
    Theoretical Occupancy                                                                %                          33.33
    Achieved Occupancy                                                                   %                           2.08
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is limited by the number of blocks that can fit on the SM. The difference 
          between calculated theoretical and measured achieved occupancy can be the result of warp scheduling           
          overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within  
          a block as well as across blocks of the same kernel.                                                          

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.50
    Branch Instructions                                                               inst                              1
    Branch Efficiency                                                                    %                              0
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------

  spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*), 2021-May-23 12:12:03, Context 1, Stream 7
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           9.43
    SM Frequency                                                             cycle/nsecond                           1.38
    Elapsed Cycles                                                                   cycle                        533,161
    Memory [%]                                                                           %                          62.44
    SOL DRAM                                                                             %                           6.68
    Duration                                                                       usecond                         385.79
    SOL L1/TEX Cache                                                                     %                          65.20
    SOL L2 Cache                                                                         %                           3.31
    SM Active Cycles                                                                 cycle                     510,343.90
    SM [%]                                                                               %                          62.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    OK    Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis report sections.                        

    OK    The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance.                                      

    Section: Compute Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Executed Ipc Active                                                         inst/cycle                           2.52
    Executed Ipc Elapsed                                                        inst/cycle                           2.41
    Issue Slots Busy                                                                     %                          62.90
    Issued Ipc Active                                                           inst/cycle                           2.52
    SM Busy                                                                              %                          62.90
    ---------------------------------------------------------------------- --------------- ------------------------------
          LSU is the highest-utilized pipeline (65.2%). It executes load/store memory operations. The pipeline is       
          well-utilized and might become a bottleneck if more work is added.                                            

    Section: Memory Workload Analysis
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Throughput                                                         Gbyte/second                          60.41
    Mem Busy                                                                             %                          43.66
    Max Bandwidth                                                                        %                          62.44
    L1/TEX Hit Rate                                                                      %                          26.49
    L2 Compression Success Rate                                                          %                              0
    L2 Compression Ratio                                                                                                0
    L2 Hit Rate                                                                          %                         100.31
    Mem Pipes Busy                                                                       %                          62.44
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Scheduler Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    One or More Eligible                                                                 %                          62.91
    Issued Warp Per Scheduler                                                                                        0.63
    No Eligible                                                                          %                          37.09
    Active Warps Per Scheduler                                                        warp                          11.53
    Eligible Warps Per Scheduler                                                      warp                           1.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Warp State Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Warp Cycles Per Issued Instruction                                               cycle                          18.32
    Warp Cycles Per Executed Instruction                                             cycle                          18.33
    Avg. Active Threads Per Warp                                                                                       32
    Avg. Not Predicated Off Threads Per Warp                                                                        30.41
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   On average, each warp of this kernel spends 10.3 cycles being stalled waiting for sibling warps at a CTA      
          barrier. This represents about 56.1% of the total average of 18.3 cycles between issuing two instructions. A  
          high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that    
          causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible    
          try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to         
          identify which barrier instruction causes the most stalls and optimize the code executed before that          
          synchronization point first.                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OK    Check the Source Counters section for the top stall locations in your source based on sampling data.          

    Section: Instruction Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Avg. Executed Instructions Per Scheduler                                          inst                     320,935.73
    Executed Instructions                                                             inst                    105,266,920
    Avg. Issued Instructions Per Scheduler                                            inst                     321,007.45
    Issued Instructions                                                               inst                    105,290,442
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                      25,640
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                            512
    Static Shared Memory Per Block                                              byte/block                            544
    Threads                                                                         thread                      3,281,920
    Waves Per SM                                                                                                    26.06
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             16
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             47
    Block Limit Warps                                                                block                             12
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          96.09
    Achieved Active Warps Per SM                                                      warp                          46.12
    ---------------------------------------------------------------------- --------------- ------------------------------
    OK    This kernel's theoretical occupancy is not impacted by any block limit.                                       

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.23
    Branch Instructions                                                               inst                     24,393,130
    Branch Efficiency                                                                    %                            100
    Avg. Divergent Branches                                                                                             0
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c7c0                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c7e0                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c7f0                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c820                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c840                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c870                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c890                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   Uncoalesced shared access, expected 455410 sectors, got 910820 (2.00x) at PC 0x7fbe0247c8a0                   

